WEBVTT

1
000:00:00.000 --> 000:00:01.150
And then, obviously, yeah.

2
000:00:02.390 --> 000:00:04.380
Greg: okay, cool.

3
000:00:04.510 --> 000:00:08.229
Greg: yes, 2 ways to do it. Also, just while we're here.

4
000:00:08.870 --> 000:00:11.170
Greg: And we can.

5
000:00:11.760 --> 000:00:12.840
Greg: let's take.

6
000:00:13.670 --> 000:00:14.490
Okay.

7
000:00:17.240 --> 000:00:19.200
Greg: So if you want to compare

8
000:00:20.400 --> 000:00:23.749
Greg: to your results. I need to, you know, yeah.

9
000:00:25.890 --> 000:00:27.480
Greg: this is a bit confusing.

10
000:00:27.920 --> 000:00:34.369
Greg: So just look at the order there. So this is just if you want to compare to results. We're just taking the results from A,

11
000:00:34.570 --> 000:00:40.400
Greg: this first select and then doing a union to the next, and then we can just see if our outputs are the same. So that's what I'm doing here.

12
000:00:40.540 --> 000:00:43.369
Greg: The reason why I'm adding this string.

13
000:00:44.420 --> 000:00:45.630
Greg: It's just

14
000:00:46.720 --> 000:00:51.430
Greg: because the output isn't necessarily in the order you'd expect. And

15
000:00:52.530 --> 000:00:59.319
Greg: so actually, it's done it all right here. But sometimes depending on the count, it might reorder these. So if you're ever doing a union, just

16
000:00:59.480 --> 000:01:05.090
Greg: I always think I make sure just to label which table we're looking at, because otherwise it gets a bit messed up.

17
000:01:05.930 --> 000:01:09.790
Greg: Okay, cool. Alex. You've got a different number in the chat

18
000:01:10.700 --> 000:01:13.210
Greg: and florid. You've got different number. I'm guessing.

19
000:01:13.870 --> 000:01:25.019
Alex Kononov: Sorry I didn't mean to to send it. Yeah. I am using Bluetooth keyboard, trying it out. And apparently there are some pro problems with

20
000:01:25.660 --> 000:01:27.219
Alex Kononov: bliss of protocol.

21
000:01:27.770 --> 000:01:28.570
Greg: Okay?

22
000:01:29.010 --> 000:01:30.480
Greg: And Lauren.

23
000:01:31.440 --> 000:01:39.779
Florine: yeah, I got that same number that Alex put in. I think because I did count. And then distinct user, crm, id

24
000:01:40.080 --> 000:01:42.040
Greg: in the user table.

25
000:01:42.730 --> 000:01:44.640
Alex Kononov: Yeah, yeah.

26
000:01:44.920 --> 000:01:53.940
Alex Kononov: I actually did. So this thing that I was talking about. But I used a Crm ids instead of Kooky ids.

27
000:01:54.250 --> 000:02:04.209
Greg: okay, cool. Can you just send me one of your queries we can put in here.

28
000:02:04.530 --> 000:02:06.049
Greg: Let's get that sleep with it.

29
000:02:11.220 --> 000:02:14.780
Greg: So I think something interesting will come up. But I didn't think it come up that quickly.

30
000:02:16.330 --> 000:02:20.100
Greg: Okay, cool. So, and I'm just gonna label this query, C,

31
000:02:21.280 --> 000:02:38.460
Greg: so yeah, all I'm doing is selecting. And I'm just adding, this is just adding another column, but obviously it's not column. It's just a string. So you can just do that, and it will add that in the column.  so let's see. Account distinct. I'll use ceremon id from where transaction count is not. No, okay, cool.

32
000:02:40.520 --> 000:02:42.859
Greg: interesting. What does this mean?

33
000:02:48.300 --> 000:02:51.250
Florine: Yeah. I thought. It then meant that this

34
000:02:51.990 --> 000:02:53.330
Florine: multiple

35
000:02:54.130 --> 000:02:58.289
Florine: transactions in there with the with the same user. Cr, m id.

36
000:02:58.530 --> 000:03:03.680
Greg: try running this. But so this should just give us the distinct number of uses.

37
000:03:05.220 --> 000:03:07.810
Alex Kononov:  

38
000:03:07.940 --> 000:03:08.720
Greg: alright!

39
000:03:10.380 --> 000:03:19.000
Alex Kononov: May I ask a question about how sickle works actually are zeroes equal to nulls or not in the sequel.

40
000:03:19.850 --> 000:03:23.500
Greg: 0 is equal to null. So if this just had a 0 in it.

41
000:03:24.620 --> 000:03:26.159
Greg: yeah, that's not an L value.

42
000:03:30.250 --> 000:03:31.950
Greg: So what are we counting?

43
000:03:33.190 --> 000:03:38.629
Greg: Sorry, I see cool. So what you're saying is in this. Okay? Right? Let's see that.

44
000:03:39.240 --> 000:03:40.110
Greg: And

45
000:03:41.850 --> 000:03:48.380
Greg: so I think what you're getting at is, there are some transaction counts where it's not null, but it's 0. So this 7 made purchase.

46
000:03:48.620 --> 000:03:53.330
Greg: So if we add this condition in where it's not now, but we're also checking. And

47
000:03:53.510 --> 000:04:02.469
Greg: but they are greater than 0. Let's just run this last little bit. And so I'm just trying to break the problem down. Okay, cool. That's interesting.

48
000:04:03.780 --> 000:04:04.950
Greg: What do we get before?

49
000:04:09.380 --> 000:04:15.240
Greg: Problem? Why do we have different numbers here? I'm quite intrigued Peter.

50
000:04:16.279 --> 000:04:21.660
Peter Shatwell:  I'm not sure how granular that the transaction

51
000:04:21.769 --> 000:04:29.389
Peter Shatwell: date, but but the first purchase date is cause, I guess when when we count just first purchase day.

52
000:04:29.790 --> 000:04:33.739
Peter Shatwell:  that's not counting distinct values right

53
000:04:35.600 --> 000:04:41.210
Peter Shatwell: again. That means that the same user could have multiple transactions on the same day.

54
000:04:41.520 --> 000:04:47.370
Peter Shatwell: And so that number could be bigger than just the distinct users.

55
000:04:51.340 --> 000:04:57.240
Peter Shatwell: Yeah, we don't expect these numbers to be the same. If if someone only made one transaction per day

56
000:05:00.590 --> 000:05:03.910
Greg: as in. If we were to look in first purchase, date

57
000:05:04.460 --> 000:05:08.110
Peter Shatwell: or last purchase day. They've made 2 purchases, but that would be the same.

58
000:05:10.490 --> 000:05:12.740
Greg: But as in 2 purchases on the same day.

59
000:05:13.130 --> 000:05:15.249
Peter Shatwell: Yeah, I think so

60
000:05:15.830 --> 000:05:20.650
Greg: interesting. So then transaction count should be greater than 0. But we know our data might not be that

61
000:05:24.950 --> 000:05:26.479
Greg: good example of it.

62
000:05:27.810 --> 000:05:32.489
Greg: So we've got the first purchase and the latest on the same day. The transaction count is too

63
000:05:33.060 --> 000:05:35.310
Greg: interesting, so

64
000:05:36.860 --> 000:05:39.959
Greg: perhaps it's a better method to look where it's greater than 0.

65
000:05:44.230 --> 000:05:47.110
Greg: I'm still intrigued. So one thing I'm just thinking about

66
000:05:48.130 --> 000:05:50.230
Greg: is that. And

67
000:05:52.230 --> 000:05:55.180
Greg: let's just simplify this a little bit.

68
000:05:57.310 --> 000:05:58.670
Greg: So

69
000:06:01.190 --> 000:06:07.300
Greg: so if we basically count the distinct number of the rm, ids, all right. Okay, fine. And then

70
000:06:08.990 --> 000:06:11.830
Greg: but one thing I want to check for is that we don't have duplicates.

71
000:06:12.490 --> 000:06:14.590
Greg: Let me just cut this out for a minute.

72
000:06:17.010 --> 000:06:18.260
Greg: save

73
000:06:21.030 --> 000:06:21.850
of

74
000:06:27.260 --> 000:06:27.940
Greg: no

75
000:06:28.680 --> 000:06:31.650
Greg: sorry this

76
000:06:33.950 --> 000:06:39.769
Greg: fun. And then also, I want to just count the number of uses itself.

77
000:06:43.230 --> 000:06:45.420
Greg: Do we have duplicate user ids.

78
000:06:47.260 --> 000:06:50.910
Greg: Yes, we do. which is an interesting find.

79
000:06:54.010 --> 000:06:56.870
Greg: which is why I think we're getting different values.

80
000:06:58.990 --> 000:07:05.800
Greg: Now, I'm wondering how this has got into this table. I don't think that's intended. which is quite an interesting find.

81
000:07:07.780 --> 000:07:09.659
Greg: That's something to look into later.

82
000:07:10.160 --> 000:07:13.010
Greg: Okay, but this is a good point, right? So

83
000:07:13.440 --> 000:07:20.929
Greg: we do need to be careful of duplicates in or use the table, and it's now making me wonder if there are other tables as well.

84
000:07:21.060 --> 000:07:22.470
Greg: This might have happened.

85
000:07:23.540 --> 000:07:26.189
Greg: I'm wondering if this happened when I copied them over the other day.

86
000:07:28.710 --> 000:07:31.030
Greg: So let's just see the same thing here.

87
000:07:33.920 --> 000:07:35.629
Greg: Sorry this has taken a turn.

88
000:07:38.180 --> 000:07:42.050
Greg: If you want to move on, please let me know. But let's just look across the session. So

89
000:07:45.140 --> 000:07:47.219
Greg: yeah, duplicates always tricky.

90
000:07:54.640 --> 000:07:56.499
Greg: I think we should be okay in this table?

91
000:07:58.960 --> 000:07:59.640
Greg: Hmm.

92
000:08:03.140 --> 000:08:05.509
Greg: oh, hang up. Why is this not gonna work?

93
000:08:08.020 --> 000:08:10.089
Greg: Okay? Yeah. But what we can check.

94
000:08:11.990 --> 000:08:19.980
Greg: Okay? So what I'm looking at is, this is the same number in the sessions. This is the same thing below user ids as when we counted.

95
000:08:21.230 --> 000:08:22.530
Greg: And hair right?

96
000:08:23.710 --> 000:08:25.840
Greg: Oh, so that is my job.

97
000:08:26.220 --> 000:08:29.279
Greg: So this is the true number. This is the number we've got in our

98
000:08:30.500 --> 000:08:34.650
Greg: users table. So basically, what I'm trying to see here is.

99
000:08:34.870 --> 000:08:42.850
Greg: I'm looking in the different table sessions. And I'm just checking. See how many distinct ids do we have versus how many ids we have. and obviously

100
000:08:43.020 --> 000:08:56.190
Greg: a person's probably gonna have more than one session. So we expect the count of, like the non-distinct values to be higher. That's fine, and the distinct values is the important thing. And this matches the distinct values in our users table.

101
000:08:57.110 --> 000:09:03.650
Greg: And however, when we count the same thing in our user table, we've got definitely have some duplicates, so

102
000:09:04.030 --> 000:09:05.080
Greg: we shouldn't.

103
000:09:05.820 --> 000:09:14.950
Greg: which actually leads me nicely into some Ddl. Which I can go into next. So we shouldn't actually have these, because if you look at how the users table are structured. The user serm Id should be unique.

104
000:09:14.970 --> 000:09:18.750
Greg: Each row should be a unique value, right? We shouldn't have duplicate rows.

105
000:09:20.380 --> 000:09:23.490
Greg: Everyone agree with that or anyone think that's not right?

106
000:09:29.920 --> 000:09:34.899
Greg: Can someone try and explain to me why is a problem that we have duplicate roads other than

107
000:09:35.170 --> 000:09:36.839
Greg: us getting the wrong values there.

108
000:09:39.700 --> 000:09:42.609
Greg: like within this table. But it might also be a problem elsewhere.

109
000:09:47.800 --> 000:09:52.580
Greg: This was a problem for the last cohort. By the way, Victoria, how you doing?

110
000:09:53.760 --> 000:10:08.180
Victoria Sanders: Well, if it's supposed to be like a database of just the users. So you can track things in different tables based on a user. Or you know, overall how many users there are. If there are duplicates, then you're just doubling up a lot of your data.

111
000:10:09.240 --> 000:10:13.689
Victoria Sanders: So it's because of the nature of the table wanting to be a list of just users.

112
000:10:14.210 --> 000:10:16.320
Greg: Yeah, and exactly

113
000:10:16.580 --> 000:10:34.230
Greg: which means that also, if you then join it to another table, and you're trying to use Id as a unique key. It's going to do 2 joins for every Id, or it's going to double, or however many more duplicates you have for every Id, and will create another row. So if you then looked at transactions and summed across revenue, your revenue is going to probably be

114
000:10:34.280 --> 000:10:35.809
Greg: 1.007times

115
000:10:36.000 --> 000:10:38.069
Greg: larger than it actually is.

116
000:10:38.630 --> 000:10:41.409
Greg: and which is such.

117
000:10:41.560 --> 000:10:42.500
Greg: Let's

118
000:10:42.610 --> 000:10:46.270
Greg: so what I want. So I was actually going to create a new users table.

119
000:10:46.380 --> 000:10:49.799
Greg: And but let's just create a new user's table without.

120
000:10:52.890 --> 000:10:55.310
Greg: has anyone heard? And then we'll do this.

121
000:10:55.390 --> 000:10:59.210
Greg: Has anyone heard of Ddl or Dml.

122
000:11:04.690 --> 000:11:16.499
Greg: okay, cool. So this is a bit more engineering stuff. Hence why, I've put advance in the title. And so again, I just want to maybe reiterate some of this stuff is a lot more complicated than maybe what you put out. We've done on day to go

123
000:11:16.540 --> 000:11:22.379
Greg: but it's quite interesting and sometimes quite useful to know. And so Ddl

124
000:11:23.790 --> 000:11:24.750
Greg: is

125
000:11:24.950 --> 000:11:28.280
Greg: data, definition, language.

126
000:11:28.880 --> 000:11:31.579
Greg: And DM, O is data

127
000:11:31.820 --> 000:11:34.670
Greg: relation. I can type

128
000:11:36.250 --> 000:11:44.279
Greg: which might sound kind of complicated. But it's really not that bad. This is basically like where you just create tables. And this is basically where you just select

129
000:11:44.680 --> 000:11:52.650
Greg: what you want. There's a couple of other types. But that's basic difference between them. And I'm just gonna drop this in the chat

130
000:11:53.070 --> 000:11:55.710
Greg: because this guy has quite a nice answer.

131
000:11:56.140 --> 000:11:59.119
Greg:  okay, so

132
000:11:59.590 --> 000:12:02.399
Greg: this is quite handy. And this leads me to

133
000:12:02.770 --> 000:12:13.469
Greg: why I've created this other data set. So if you want to create your tables or save your data, and if you've joined lots of tables together, and you want to put that in a dashboard. We tend to use views for that.

134
000:12:13.670 --> 000:12:21.489
Greg: as I think I went over before. But I'll go over again. So we've got tables over here like like funnels or users.

135
000:12:21.510 --> 000:12:33.510
Greg: and we can preview that table. It's built the table. The data's been stored in here with a view it doesn't actually store the data. It just kind of saves the query. And then, when you query that view. If you select all from that view just like you would a table.

136
000:12:33.930 --> 000:12:34.750
Greg: and

137
000:12:36.540 --> 000:12:48.620
Greg: then it then basically pulls all the most recent data, runs the query again. So if you're building a dashboard, and you want to say I don't know. Look at all the sessions, and then join that to transactions to get the revenue.

138
000:12:48.680 --> 000:12:56.230
Greg: then you might want to create a view and connect your power. VR. Dashboard, to your view, so that every time you run your dashboard it reruns the query. And you get fresh data.

139
000:12:56.430 --> 000:13:02.909
Greg: So that's one type of thing I'm I've got a lot of attention. But what we want to do is basically create

140
000:13:03.590 --> 000:13:09.480
Greg: table. And then you can also do create view. We want to create a new table.

141
000:13:11.130 --> 000:13:14.560
Greg: And we're gonna call this, let's just get rid of this for a sec.

142
000:13:17.430 --> 000:13:18.190
Greg: And

143
000:13:18.370 --> 000:13:20.540
Greg: and we're gonna call this users.

144
000:13:21.700 --> 000:13:24.910
Greg: It's gonna throw. And over it. what am I doing wrong?

145
000:13:27.210 --> 000:13:29.320
Karum Singh: There's radio table called users.

146
000:13:29.970 --> 000:13:31.209
Greg: But what's the error?

147
000:13:31.610 --> 000:13:32.760
Karum Singh: Well, I'm not sure

148
000:13:35.500 --> 000:13:36.790
Rami: the degradation

149
000:13:37.440 --> 000:13:40.230
Greg: attention got to use the back tips

150
000:13:41.220 --> 000:13:48.590
Greg: and okay, cool. And then it says it's got to be qualified with the data set. Where are you? What are you talking about? So I need to tell that to the warehouse.

151
000:13:49.340 --> 000:13:50.640
Greg: and then I can do

152
000:13:52.110 --> 000:13:53.530
Greg: as oops.

153
000:13:54.070 --> 000:14:02.959
Greg: And what this will do is it will create a table. We'll get an error here because this table already exists exactly where I can. So we sometimes write or replace.

154
000:14:03.160 --> 000:14:09.749
Greg: And so this is just going to create or replace this table. And and then, as and then it's just your query here.

155
000:14:11.100 --> 000:14:13.890
Greg: And then I'm gonna select

156
000:14:16.260 --> 000:14:17.400
Greg: daisies.

157
000:14:17.870 --> 000:14:20.460
Greg: But we want to get rid of duplicates.

158
000:14:21.240 --> 000:14:27.459
Greg:  so how should I write this statement?

159
000:14:28.480 --> 000:14:30.390
Greg: And then this is just going to create a table.

160
000:14:30.720 --> 000:14:39.269
Greg: But if I basically, what I'm trying to do is I'm gonna recreate the users table over here. But I want to get rid of all the duplicates. Any idea how we might write that query.

161
000:14:40.040 --> 000:14:42.989
Greg: because right now it's just going to count everything. Yeah, like

162
000:14:43.580 --> 000:14:52.740
Alex Kononov: you could filter the unique ids for the users and then left. Join on them

163
000:14:52.970 --> 000:14:54.430
Alex Kononov: the same table.

164
000:14:55.580 --> 000:14:59.780
Greg: Okay. Does everyone want to take 2 min to write a query

165
000:15:00.190 --> 000:15:02.970
Greg: and put it in the chat actually, might be a good practice.

166
000:15:05.690 --> 000:15:07.280
Greg: That's an interesting way of doing it.

167
000:15:11.300 --> 000:15:12.320
Greg: So

168
000:15:16.200 --> 000:15:20.410
Greg: maybe take a well, yeah, take a couple minutes, and then, if you've got one, pop it in the chat.

169
000:15:22.650 --> 000:15:27.200
Greg: because that's an interesting way of doing it. There's always lots of ways to do stuff

170
000:15:29.520 --> 000:15:34.619
Eli Azlin: just to double check. You just want to get rid of all of the duplicate users right?

171
000:15:34.670 --> 000:15:39.349
Greg: Exactly. So what we're going to do is we're going to select all the columns and everything from here.

172
000:15:39.940 --> 000:15:43.310
Greg: So if I were to slap so reason why we can't do

173
000:15:46.090 --> 000:15:48.099
Greg: this. Oh, my!

174
000:15:48.560 --> 000:15:49.350
Greg: And

175
000:15:49.590 --> 000:15:51.999
Greg: was that 8 min here and Mit.

176
000:15:53.300 --> 000:15:55.879
Greg: It's because if I run this, it's

177
000:15:57.060 --> 000:15:57.960
Greg: get rid of that.

178
000:16:00.010 --> 000:16:07.170
Greg: This is what it would then replace the table with. It's it's your output. And so if I did select distinct of that, we would replace the whole table with just the Ids

179
000:16:11.560 --> 000:16:12.320
Greg: cool.

180
000:16:13.970 --> 000:16:16.570
Greg: Jack's put in the chat. Can you select.

181
000:16:19.490 --> 000:16:21.120
Greg: if everyone else. Another second

182
000:16:23.150 --> 000:16:23.940
second.

183
000:16:25.940 --> 000:16:27.830
Greg: let's see what happens if we do that

184
000:16:43.820 --> 000:16:46.020
Greg: now, can we confirm, if this is right or not.

185
000:17:01.540 --> 000:17:04.240
Greg: any ideas on how to check if this output is right.

186
000:17:05.240 --> 000:17:08.279
Greg: So what we've done is we've just select distinct across all the rows

187
000:17:11.760 --> 000:17:16.089
Victoria Sanders: do the same count distinct customer ids

188
000:17:16.260 --> 000:17:18.220
Victoria Sanders: versus just count.

189
000:17:18.490 --> 000:17:20.699
Greg: Yeah. So it's what we did over here. Right?

190
000:17:21.180 --> 000:17:25.099
Greg: So this was our distinct value. And then this was the duplicate.

191
000:17:25.670 --> 000:17:27.589
Greg: So yeah. oh.

192
000:17:28.329 --> 000:17:31.149
Greg: it's mixed up to 101185.

193
000:17:32.910 --> 000:17:35.710
Greg: So you still have. How can it be?

194
000:17:38.210 --> 000:17:41.590
Greg: I expected that to to work and be the same number? Actually.

195
000:17:48.890 --> 000:17:50.100
Greg: that's interesting.

196
000:17:57.640 --> 000:18:17.429
Greg: Okay? So maybe while people are still giving a go, because that's an interesting thing to talk about. By the way, that's all this session is, it's just finding stuff that's interesting to talk about. I mean, interesting from like a day at the point of view. So I don't know how interesting, that is. But you know, for us, it's gonna be a learning experience. And so what this is doing is is basically distinct across all the rows. So

197
000:18:17.870 --> 000:18:18.840
Greg: if

198
000:18:20.160 --> 000:18:23.779
Greg: how do I read this? So if we take, let's go and hit.

199
000:18:24.960 --> 000:18:36.929
Greg: So if we go across all the rows, basically, if if everything in here is all of these need to be unique, for it to be distinct. If even one column changes, it's not counted as distinct. So it's looking across all the rows.

200
000:18:38.880 --> 000:18:41.050
Greg: So I'm interested as to where

201
000:18:45.210 --> 000:18:48.909
Greg: those like 400 duplicate rows are coming from.

202
000:18:50.530 --> 000:18:52.129
Eli Azlin: And is it

203
000:18:52.710 --> 000:19:00.210
Eli Azlin: I was? Gonna say, does it make a difference if you you've collected, if you've put distinct, like all the columns, because in the

204
000:19:00.270 --> 000:19:06.860
Eli Azlin: the one where we were checking the duplicate numbers, I think we only checked for distinct user. CRMI, d.

205
000:19:08.640 --> 000:19:10.840
Eli Azlin: So one of them makes a difference.

206
000:19:11.160 --> 000:19:14.850
Greg: So we've got a slightly different number. So there is. But I'm not quite sure.

207
000:19:16.690 --> 000:19:22.800
Greg:  Why, that is, can we take an except

208
000:19:26.310 --> 000:19:27.820
Greg: I haven't been listening

209
000:19:33.510 --> 000:19:34.320
a

210
000:19:36.350 --> 000:19:39.479
Greg: sorry. So what I'm thinking is, basically, I wanted to try and

211
000:19:39.860 --> 000:19:44.720
Greg: look at the rows from this result, and then compare them with the rows from the other result.

212
000:19:46.580 --> 000:19:51.580
Greg: but yeah. So I think there is a slight difference, because if we just look at distinct here.

213
000:19:51.660 --> 000:19:52.690
Greg: we get

214
000:19:54.020 --> 000:19:55.240
Greg: this many right

215
000:19:55.400 --> 000:19:59.759
Greg: across all the rows. We seem to get about 400 more

216
000:20:00.120 --> 000:20:05.419
Greg: or 460 more. So I'm guessing what's happening is that like we have

217
000:20:06.560 --> 000:20:13.049
Greg: some duplicate ids that everything is the same. Apart from maybe one of these columns.

218
000:20:14.530 --> 000:20:18.730
Greg: So what could have happened? Perhaps, as we had like duplicates for everyone.

219
000:20:18.840 --> 000:20:29.459
Greg: And then there's about 400 people where we have a duplicate road, but something about them has changed. Maybe they made a purchase when the previous duplicate they hadn't. Maybe they've like changed the prison status or something like that.

220
000:20:29.490 --> 000:20:34.419
Greg: So yeah, a better way to do it is to to basically do distinct all the Ids.

221
000:20:36.710 --> 000:20:38.290
Greg: How are we feeling on this problem?

222
000:20:43.020 --> 000:20:47.830
Greg: Do you want to put a one to 5, and I've been good one being. I have no idea how to solve it.

223
000:20:48.360 --> 000:20:51.549
Greg: and you can just DM me if you don't want to put it to everyone.

224
000:20:57.130 --> 000:21:02.939
Greg: it's a slightly calculation. and this one will come with me. can I? Yeah, me, can I ask a question.

225
000:21:03.500 --> 000:21:09.710
Florine: So if you wanted to figure out what the issue was. and to see where

226
000:21:10.650 --> 000:21:17.330
Florine: there are duplicates in rows. would you just do like select all

227
000:21:18.840 --> 000:21:21.850
Florine: from the user table and then sorted by like

228
000:21:23.200 --> 000:21:24.879
Florine: Serum Id and

229
000:21:25.440 --> 000:21:31.190
Florine: sending order. And then you can check. Or how would you do that? So I think I, basically.

230
000:21:32.100 --> 000:21:46.840
Rami: I think I think doing distinct U user Id is going all to follow the same column. It's going all to follow user id when you do this thing is gonna be the same if you do a distinct or

231
000:21:47.980 --> 000:21:52.920
Greg: but it's not the same because we're looking at the. So we get a slightly different number

232
000:21:52.930 --> 000:21:58.129
Greg: when we look at the distinct of all across all the columns to versus just the Id.

233
000:21:58.360 --> 000:21:59.809
Greg: So if I change

234
000:22:01.880 --> 000:22:02.870
Greg: dance.

235
000:22:05.740 --> 000:22:07.170
Greg: state account phone.

236
000:22:09.020 --> 000:22:09.810
Greg: And

237
000:22:17.910 --> 000:22:19.819
Greg: alright, yeah, there's no you do that at all.

238
000:22:22.260 --> 000:22:23.120
Greg: Okay, bye.

239
000:22:28.640 --> 000:22:30.509
Greg: that's just okay.

240
000:22:30.760 --> 000:22:37.750
Greg: I'm going to take this out into a new tab, because it looks a little bit confusing here. So basically, I'm going to look at

241
000:22:38.710 --> 000:22:42.920
Greg: create one table right where we've looked at distinct of all the columns.

242
000:22:44.680 --> 000:22:47.089
Greg: And then I'm gonna create another table.

243
000:22:51.820 --> 000:22:53.590
Greg: But why would that?

244
000:22:55.030 --> 000:22:56.309
Greg: And then just like.

245
000:22:58.430 --> 000:22:59.330
Greg: And

246
000:23:02.750 --> 000:23:05.900
Greg: so what we need to do is like, break this problem down.

247
000:23:11.840 --> 000:23:17.120
Greg: So what I'm trying to do is basically see? Like where the difference is, we've got one result here and then one result there.

248
000:23:18.340 --> 000:23:20.250
Greg: And we want to understand.

249
000:23:27.020 --> 000:23:27.690
And

250
000:23:29.530 --> 000:23:32.149
Greg: so we want to understand, like the difference between them.

251
000:23:46.680 --> 000:23:51.460
Greg: So if I just do that right, so this is the one we've done distinct across all the rows.

252
000:23:56.630 --> 000:23:58.969
Greg: So you get a slightly different number

253
000:24:00.220 --> 000:24:01.740
Greg: versus when we do.

254
000:24:03.260 --> 000:24:14.110
Greg:  a slightly different number versus when we just look at distinct of all the crm, ids.

255
000:24:15.140 --> 000:24:19.330
Greg: so there must be some rows in here for some reason which are duplicated.

256
000:24:21.050 --> 000:24:29.720
Greg: So yeah, if you really want it, this is probably a bit of investigation. So I'd I'd probably like, go into here and try and pick out

257
000:24:29.730 --> 000:24:30.820
Greg: the rose.

258
000:24:31.500 --> 000:24:33.189
Greg: And and I,

259
000:24:34.880 --> 000:24:39.450
Greg: yeah. So I think the first thing I'd want to do is create this table without any duplicates.

260
000:24:40.450 --> 000:24:44.460
Florine: Would that be an easy way of identifying those rows?

261
000:24:45.150 --> 000:24:48.629
Greg: So what I used to do when we were testing tables.

262
000:24:48.670 --> 000:24:57.090
Greg: Is, you could take table A and you could select tool and then do an accept statement from select tool from Table B, and basically, what it will do is it will

263
000:24:57.350 --> 000:25:01.400
Greg: only produce the rows that don't exist. So if you take Table A

264
000:25:01.780 --> 000:25:06.439
Greg: and do accept B, it will take away all the rows that are the same, so it will give you

265
000:25:07.320 --> 000:25:09.769
Greg: all the rows in A that don't exist in B,

266
000:25:09.990 --> 000:25:14.859
Greg: and then the same be accept because you will, rose and be there on an A,

267
000:25:15.150 --> 000:25:16.010
Greg: and

268
000:25:17.740 --> 000:25:20.490
Greg: so hang on a couple of things in the chat.

269
000:25:28.560 --> 000:25:30.719
Greg: Okay, cool. Let's try.

270
000:25:31.830 --> 000:25:32.930
Greg: Basically.

271
000:25:34.310 --> 000:25:40.259
Greg: I'd probably avoid using the funnel events table because we don't know what's going on in that table, either.

272
000:25:40.850 --> 000:25:44.100
Greg: So I think I would just stick with the but one we're currently on.

273
000:25:44.350 --> 000:25:50.159
Greg: So I'm gonna try this one and just see what we get. So let's walk through this.

274
000:25:52.240 --> 000:25:58.890
Greg: So selecting all from our users table. And this is gonna be. This is new in a joint.

275
000:26:00.400 --> 000:26:01.440
Greg: So

276
000:26:03.700 --> 000:26:06.060
Greg: this is where we've got the distinct Id.

277
000:26:06.800 --> 000:26:11.020
Greg: and then just joining on the Id. Let's see if that works.

278
000:26:18.000 --> 000:26:19.530
Greg: Did you test this? Peter?

279
000:26:22.070 --> 000:26:22.740
Okay.

280
000:26:23.220 --> 000:26:30.070
Greg: yeah, I tried. But yeah, I said, it just looks like you get a table with the same number of rows roses before. So I'm not sure

281
000:26:30.250 --> 000:26:31.480
Peter Shatwell: why I'm

282
000:26:31.700 --> 000:26:34.839
Greg: yeah expecting a smaller number

283
000:26:35.200 --> 000:26:37.510
Greg: complicated. And

284
000:26:38.280 --> 000:26:43.990
Greg: yeah, so oops sorry to delete that. So the reason why that's happening is because you've got the same Ids from here.

285
000:26:44.220 --> 000:26:50.179
Greg: And then this is gonna be the same table. So what's happening is you're just joining on the Ids, because they exist in both tables.

286
000:26:50.260 --> 000:26:53.959
Greg: The inner join still just produces the same thing. If that makes sense.

287
000:26:54.800 --> 000:26:58.080
Greg: because you've this is this is everything from the current table, isn't it?

288
000:26:59.630 --> 000:27:04.680
Greg: And then here you try to distinct the select Ids, which is fine. So you've got all unique ids.

289
000:27:05.670 --> 000:27:11.279
Greg: But then you've done an inner join, and it's still the same unique ids here and in that table that you're joining on.

290
000:27:11.490 --> 000:27:16.670
Greg: So they still exist in both tables. So it's still going to join. So for every id here.

291
000:27:16.780 --> 000:27:22.500
Greg: as long as this is in this table, it was still joined on it. and it's gonna bring it in

292
000:27:24.920 --> 000:27:28.310
Greg: so that it'd only work if the Ids were different.

293
000:27:29.240 --> 000:27:39.120
Greg: It's a bit of a tricky one. But basically like this is the initial table. And as you're joining on the Ids which are in both tables, it's the inner join works fine, and you're still gonna end up with the same amount.

294
000:27:40.760 --> 000:27:43.679
Peter Shatwell: So what sort of join do do we want then?

295
000:27:43.970 --> 000:27:47.029
Greg: I don't know if I'd do a joint Victoria.

296
000:27:49.110 --> 000:27:54.110
Victoria Sanders: if you did want to do it with a join. Could you get the list of the distinct

297
000:27:54.230 --> 000:28:07.929
Victoria Sanders: Ids, and then left? Join the rest of the information from the original table back to it. So then you're only getting what's in that initial list of the unique Ids.

298
000:28:09.520 --> 000:28:10.880
Victoria Sanders: Maybe

299
000:28:10.930 --> 000:28:26.649
Greg: I would need you to write me that query. Yeah, if you write that I could take a look. It's an interesting way to do it. I don't like using joins when I can, so if I'll go through if I can remember our window function from last time. So do you remember row number?

300
000:28:30.320 --> 000:28:31.729
Greg: We did it last week?

301
000:28:31.920 --> 000:28:34.649
Greg: How do we write a window function for our number.

302
000:28:36.460 --> 000:28:39.219
Greg: Give it a go, and then put it in the chat. See if you can remember.

303
000:28:51.650 --> 000:28:54.699
Greg: this is the tricky problem. by the way.

304
000:28:57.490 --> 000:29:03.400
Greg: is more tricky than I thought it was. I thought the select distinct. All was gonna work fine.

305
000:29:04.510 --> 000:29:13.879
Greg: which, like also some data engineers might actually just say, Hey, you know what? It's only 400 rows in 100,000. I don't really care, and they'd probably just push that through and leave those duplicates.

306
000:29:14.500 --> 000:29:16.520
Greg: Sometimes people are like. Not that bothered.

307
000:29:18.430 --> 000:29:19.790
Greg: Maybe we should do it correctly.

308
000:29:22.280 --> 000:29:23.689
Greg: Drop me a message if

309
000:29:24.290 --> 000:29:34.620
Eli Azlin: do you wanna do the row number? Are you rank. Well, not ranking. But are you assigning that like ordering it by the user? Crm, id.

310
000:29:34.690 --> 000:29:36.400
Eli Azlin: exactly. Yeah.

311
000:29:36.500 --> 000:29:44.329
Greg: So what I wanna do is basically select and create a window function, and we're gonna order by the Cr M. Id, so that for every

312
000:29:44.770 --> 000:29:55.570
Greg: every So I'm id. We'll go row number one and a row. Number 2. If there's a duplicate or a row. Number 3, if it's duplicated 3 times. So what we'll do is we'll basically select all the data again.

313
000:29:56.210 --> 000:30:01.419
Greg: create that row number. And we're just going to select where the row number equals one based on user serum Id.

314
000:30:01.510 --> 000:30:04.940
Greg: So for every id, we'll just get the first instance.

315
000:30:06.300 --> 000:30:11.950
Greg: which again might come with some other problems. And but let's not get into that.

316
000:30:17.000 --> 000:30:18.870
Florine: Sorry I have one more question.

317
000:30:19.090 --> 000:30:26.219
Florine: If you do that if you do that, and it say there was one id to add like 3

318
000:30:26.590 --> 000:30:29.649
Florine: rows in that table, and you just

319
000:30:29.850 --> 000:30:38.609
Florine: pick out the first row, and not the second and the third, then. But what if they had like 10 and 20 transaction counts? Is your transaction count completely?

320
000:30:39.710 --> 000:30:45.670
Greg: Exactly. It gets really confusing. So this is like, definitely a much more of an engineering problem.

321
000:30:45.820 --> 000:30:56.140
Greg: And and that's exactly the question you need to think about right? So well, actually, how do we know if that first, or even is the bright one? So what you could do is when we write our partition by

322
000:30:57.150 --> 000:30:58.639
Greg: you could, Addie. But

323
000:31:00.140 --> 000:31:02.709
Greg: so let's just do that.

324
000:31:05.160 --> 000:31:10.250
Greg: And I'm gonna just double check. I'm doing the right thing. So we need to think about

325
000:31:10.640 --> 000:31:12.639
Greg: what do we actually want to?

326
000:31:14.220 --> 000:31:15.840
Greg: Does this not have brackets?

327
000:31:18.930 --> 000:31:20.869
Greg: Why am I going mantle? What would I know?

328
000:31:20.980 --> 000:31:27.680
Greg: Alright, I need to buy? And so, yeah, it depends what you order by here. So I would actually probably order by

329
000:31:29.220 --> 000:31:33.169
Greg: yeah, let's let's use your example. Right? So let's look at transaction counts. So

330
000:31:34.170 --> 000:31:41.099
Greg: he might be right. So we what we're looking at. Okay. So what we're looking at here is. and I'll just run that.

331
000:31:42.360 --> 000:31:43.709
Greg: Why am I doing anything wrong.

332
000:31:44.750 --> 000:31:47.560
Jack C: Oh, is it just row number number, is it? Row number?

333
000:31:48.750 --> 000:31:50.599
Greg: My abbreviation is right now.

334
000:31:50.920 --> 000:31:54.949
Greg: Okay, so let's run this. And you can see what Helen talking about.

335
000:31:55.420 --> 000:31:58.370
Greg: But basically, what we want to do is create those little partitions.

336
000:31:58.460 --> 000:32:03.970
Greg: Let me just make this look a little bit nicer

337
000:32:05.850 --> 000:32:06.600
Greg: and

338
000:32:14.920 --> 000:32:15.690
Greg: okay.

339
000:32:20.890 --> 000:32:23.320
Greg: cool. There was something here.

340
000:32:23.410 --> 000:32:26.889
Greg: Okay. So actually, that's just

341
000:32:27.920 --> 000:32:28.949
Greg: select tool.

342
000:32:30.410 --> 000:32:31.879
Greg: Let's see what's really got up.

343
000:32:33.020 --> 000:32:37.230
Greg: Yeah, so this is, this is definitely a bit of a tricky, a tricky one to look at.

344
000:32:37.480 --> 000:32:42.890
Greg:  this is very much. I would say, advance equal.

345
000:32:43.260 --> 000:32:49.720
Greg: And that's what we're here for. Okay, great. So we can see here. We've got our partitions right? So

346
000:32:49.800 --> 000:32:55.190
Greg: we've partitioned by user 0, my D, and I've ordered by that. So they're all nicely together. And you can see our partitions.

347
000:32:55.510 --> 000:33:02.360
Greg: and then we hold it by transaction. Count. This is a so let's look at this first, this guy, right? These are duplicates.

348
000:33:02.600 --> 000:33:05.889
Greg: So they're exactly the same by the looks of it, these rows.

349
000:33:05.980 --> 000:33:09.760
Greg: So it doesn't matter whether I select row one or row 2, and

350
000:33:09.950 --> 000:33:17.499
Greg: I don't know if we're going to see an example of it. But what fluorine was so getting at was, if we were to look at

351
000:33:17.840 --> 000:33:21.269
Greg: okay. if we're to look at this person.

352
000:33:21.740 --> 000:33:29.500
Greg: and we see blah blah blah! This is all fine, but maybe they had a different transaction, count, which one would, you actually know is the correct one.

353
000:33:29.940 --> 000:33:32.220
Greg: And so what you could actually do is, it will

354
000:33:32.270 --> 000:33:39.980
Greg: maybe we change this order by latest purchase date, and then we would select the one with the most recent purchase date by the most recent version of the row.

355
000:33:41.050 --> 000:33:45.639
Greg: Does that make sense? Please tell me if it doesn't cause? This is quite. I know this is quite tricky.

356
000:33:46.580 --> 000:33:48.279
Greg: Please ask all the questions.

357
000:33:48.530 --> 000:33:54.470
Florine: It does make sense. But who would you in that case asked that question to

358
000:33:56.010 --> 000:34:07.149
Florine: to be like, okay, I think, we should select it on latest purchase date. Who like we can, we can make that decision. But should we? Who do we check that with? If that's the right decision to make?

359
000:34:07.220 --> 000:34:12.600
Greg: It's up to you. It depends on your analysis, right? And you're gonna be. Who who knows the data best.

360
000:34:12.929 --> 000:34:16.660
Greg: So in this case you probably know the date of birth. But maybe someone works.

361
000:34:16.980 --> 000:34:28.120
Greg: You might go to an engineer in this case and be like, is there any reason this could be wrong? But for the most part I think it's a safe assumption. Say, if we've got duplicates, but one of them has another transaction, the other one doesn't.

362
000:34:28.840 --> 000:34:31.990
Greg: I would probably go for the most recent one that has 2 transactions.

363
000:34:32.290 --> 000:34:33.500
Florine: Yeah, okay.

364
000:34:33.600 --> 000:34:35.239
Greg: might be missing on data.

365
000:34:36.949 --> 000:34:49.649
Alex Kononov: I wanted to ask if we could make a flag to identify the number of these different so to say that the duplicates that are not really duplicates.

366
000:34:49.659 --> 000:34:54.910
Alex Kononov: and you know, by having the

367
000:34:55.320 --> 000:35:00.370
Alex Kononov: I don't know difference counter between the

368
000:35:00.380 --> 000:35:11.630
Alex Kononov: number in the first row, and the second row, etc., through window function, or it is a bit too bulky to

369
000:35:11.880 --> 000:35:21.390
Greg: for a window function. I would probably just look like select from here. And look at that. I'm not gonna go into that because, yeah, you could look and see how many duplicates reach there out.

370
000:35:21.400 --> 000:35:24.340
Greg: And I mean, you could just select Max a row number

371
000:35:24.350 --> 000:35:27.419
Greg: and then see it's great. Then, too,

372
000:35:27.700 --> 000:35:36.750
Greg: yeah, I would. Basically. What I've done here is I've created the cte, which is our output, and then I'm just gonna select everything from our cte. where I'll go right on. There

373
000:35:36.770 --> 000:35:44.330
Greg: is equal to one I'm also just going to change it. So I'm going to order by the latest. I see.

374
000:35:45.600 --> 000:35:50.689
Greg: Hello! What if they haven't got a purchase date shouldn't matter right

375
000:35:52.950 --> 000:35:54.350
Greg: what's going on.

376
000:35:55.810 --> 000:35:56.650
Greg: I think.

377
000:35:57.220 --> 000:36:08.030
Greg: and then we'll descend it. So basically, what I'm going to do here just to go over this again. We're selecting all the rows, creating a row number, and we're partitioning by each user. Cr, m id, so we get our duplicates next to each other.

378
000:36:08.750 --> 000:36:14.090
Greg: which then allows it to count the row number. We're just ordering those partitions by the latest

379
000:36:14.500 --> 000:36:15.700
Greg: purchase date.

380
000:36:15.830 --> 000:36:26.089
Greg: So if they've made a purchase, or they've made 2, and one's more recent because we've ordered it by descending. The first will be the most recent purchase. The second will be

381
000:36:26.190 --> 000:36:27.619
Greg: the older purchase

382
000:36:27.850 --> 000:36:34.989
Greg: most cases they're just duplicates, and there's no difference again, if it's a null and it probably won't matter. But

383
000:36:35.260 --> 000:36:40.040
Greg: yeah, But yeah, just in case. And so if we then select.

384
000:36:44.280 --> 000:36:46.090
Greg: let's just see if this works.

385
000:36:47.550 --> 000:37:06.830
Greg: So we should get the first one. And this is how we'd handle duplicates. But we would have like 100 different things, about 20 different things that could be duplicated. And so you'd add more stuff here. Alright, we have the right number. This took much longer than I thought it would. Okay? And then the final little bit. And I'll let you take a break off. This is then, our data definition language.

386
000:37:07.080 --> 000:37:13.880
Greg: So let's take our users table and then overwrite it with this output. I'll be all happy with this output.

387
000:37:20.120 --> 000:37:21.269
Greg: Do you want this?

388
000:37:25.320 --> 000:37:27.270
Eli Azlin: We just want to get rid of the pop.

389
000:37:28.130 --> 000:37:29.460
Eli Azlin: Yeah.

390
000:37:29.740 --> 000:37:31.479
Greg: sorry I'm big, annoying.

391
000:37:31.790 --> 000:37:33.839
Greg: but sake, Greg, let us have them win.

392
000:37:33.940 --> 000:37:36.420
Greg: And oh, no. let's do that.

393
000:37:37.680 --> 000:37:39.239
Greg: Why can't I do that.

394
000:37:40.550 --> 000:37:41.779
Greg: It's in the comma.

395
000:37:43.230 --> 000:37:48.090
Greg: Okay, cool, really handy thing. It's only in bigquery. You can actually exclude columns from the output.

396
000:37:50.420 --> 000:37:52.130
Greg: I don't mind that. Apparently

397
000:37:52.590 --> 000:38:00.950
Greg: right. So we're gonna take this and then overwrite the user table. Hopefully, we're happy with that, because otherwise we're gonna lose all our data. Here. We go. Goodbye users table.

398
000:38:01.460 --> 000:38:06.499
Greg: and then it'll create a new one. So this is our Ddl definition language which is just the create or replace.

399
000:38:06.650 --> 000:38:09.620
Greg: And there you go. So place a table.

400
000:38:11.900 --> 000:38:13.649
Greg: That row number should have changed.

401
000:38:14.850 --> 000:38:16.470
Greg: Where is the refresh button?

402
000:38:17.850 --> 000:38:21.590
Greg: There we go. Okay, like. So now we can go back

403
000:38:21.740 --> 000:38:32.870
Greg: the original query. And then, hopefully, like these all give the same result. I'm gonna lose my mind. Hooray! Cool, very good. Well identified.

404
000:38:33.210 --> 000:38:37.189
Greg: Good job, guys. Thank you for helping me gather do we need a break?

405
000:38:38.870 --> 000:38:40.529
Greg: I think we might need a break.

406
000:38:42.710 --> 000:38:43.590
Florine: Yeah.

407
000:38:43.800 --> 000:38:45.300
Greg: I'm going to say we need a break.

408
000:38:45.640 --> 000:38:46.790
Greg: Okay, cool.

409
000:38:46.880 --> 000:38:50.910
Greg: Oh, I've just been some. Okay. I will see you back here in 10,

410
000:38:52.110 --> 000:38:54.320
Greg: and then maybe just go over a couple more things.

411
000:39:01.900 --> 000:39:10.619
Greg: Oh, okay, we're emphasizing that actually turned into a data engineering problem. I really thought the select distinctive, all would work.

412
000:39:10.920 --> 000:39:16.960
Greg: lesson is to check that stuff beforehand. But yeah, so this sort of thing, like what we just did is very much

413
000:39:17.210 --> 000:39:24.189
Greg: a data engineer's job is to like, Oh, there's a problem you need to go break it down, look in all the different like areas.

414
000:39:24.390 --> 000:39:30.489
Greg: try and figure out what's going on, and even talk to different. like owners of the data go around. The business

415
000:39:30.510 --> 000:39:40.210
Greg: can be quite a long thing. And yeah, this sort of query is something you'd see quite a lot written by data engineers. So absolutely not an expectation to know this stuff.

416
000:39:40.400 --> 000:39:41.280
Greg: And

417
000:39:42.460 --> 000:39:43.560
Greg: but yeah.

418
000:39:43.610 --> 000:39:45.619
Greg: hopefully, it was at least kind of interesting.

419
000:39:46.090 --> 000:39:58.039
Greg: And I explained it somewhat clearly. Please send me fit in but yeah, so a little bit more complicated than what I'd hoped to go through, and what I'd actually just kind of wanted to go through was sort of like this, create or replace.

420
000:39:58.210 --> 000:39:59.100
Greg: And

421
000:39:59.960 --> 000:40:04.770
Greg: yeah, yeah, I'll I'll put it in slack as well after. But why not put it here, too.

422
000:40:05.080 --> 000:40:06.230
Greg: So

423
000:40:07.090 --> 000:40:12.300
Greg: one thing I was going to say is, I've created this data set. You should all have edit access.

424
000:40:12.600 --> 000:40:14.670
Greg: so you can use this

425
000:40:16.710 --> 000:40:18.410
Greg: well to me, Craig's

426
000:40:20.650 --> 000:40:21.510
Greg: get out of here.

427
000:40:22.830 --> 000:40:24.010
Greg: get out of here.

428
000:40:25.200 --> 000:40:28.940
Greg: Where am I? Okay, yeah. So you can use this sort of thing. And

429
000:40:29.070 --> 000:40:31.560
Greg: you could then basically just change this.

430
000:40:35.370 --> 000:40:37.680
Greg: And then, if I were to run that

431
000:40:38.610 --> 000:40:41.840
Greg: it should create one in here, you should all be able to do that.

432
000:40:41.930 --> 000:40:46.809
Greg: If this is a little bit unfamiliar, or just can't be bothered to type it.

433
000:40:47.210 --> 000:40:50.139
Greg: Easier way to do it is actually to go to a query settings.

434
000:40:50.490 --> 000:40:54.319
Greg: Think I might have showed this last time that you can set a destination table for your results.

435
000:40:54.940 --> 000:40:58.760
Greg: and then just set that to what do you want it to be.

436
000:41:00.500 --> 000:41:14.869
Greg: and then, like whatever name I've just prefixed it with your name, Alex, you're just on my screen. But then table whatever. and just so you know it's yours, cause it might get quite busy in that data set. And when we start apply I'll create different ones for your teams as well.

437
000:41:14.910 --> 000:41:17.760
Greg: And but yeah, hopefully, it's a nice little thing to just pay a hundred.

438
000:41:17.920 --> 000:41:18.630
And

439
000:41:19.780 --> 000:41:21.320
Greg: okay. So

440
000:41:22.600 --> 000:41:31.750
Greg: we've only got 20 min left. And so that was quite complicated. So I don't know if I want to get into the other stuff so much, but I guess we might as well just finish these questions so.

441
000:41:32.210 --> 000:41:39.750
Greg: or give you something to think about just before you go? But any idea on these 2? Did anyone have a chance to try to sell these ones?

442
000:41:41.780 --> 000:41:43.110
Greg: What we might have done.

443
000:41:44.700 --> 000:41:47.490
Greg: Also, my computer is taking off, and I don't know why

444
000:42:05.470 --> 000:42:10.960
Greg: this for purchase within the last month. God! It looks so like complicated, and

445
000:42:11.670 --> 000:42:14.430
Greg: in the mainstream chat compatibility.

446
000:42:15.650 --> 000:42:18.360
Greg: She wants to walk us through what you've done here to get

447
000:42:26.400 --> 000:42:27.440
Greg: I don't know.

448
000:42:30.390 --> 000:42:31.669
Greg: Oh, I can't hear you

449
000:42:36.470 --> 000:42:37.730
Juliette: spring screen.

450
000:42:37.750 --> 000:42:39.030
Greg: Not only

451
000:42:42.020 --> 000:42:45.069
Juliette: can you hear me? Okay?

452
000:42:45.510 --> 000:42:47.809
Juliette: What was the question again, please.

453
000:42:47.840 --> 000:42:51.309
Juliette: Could you just walk through the query that?

454
000:42:51.550 --> 000:43:03.119
Juliette: Yes. So I wanted the last month and last year of the of the table. So that's what the city is.

455
000:43:03.870 --> 000:43:04.720
Greg: Umhm.

456
000:43:04.920 --> 000:43:09.390
Juliette: And then I wanted to select count

457
000:43:09.530 --> 000:43:15.560
Juliette: for all the latest purchase date was

458
000:43:15.900 --> 000:43:18.360
Juliette: the last month and the last year.

459
000:43:20.620 --> 000:43:24.530
Greg: Okay, nice. It's good. Nice there. Oh.

460
000:43:24.940 --> 000:43:29.540
Greg: that's quite a nice way to do it. Yeah, okay, cool. So you've basically just truncated to pull out the

461
000:43:30.070 --> 000:43:31.760
Greg: right. And then.

462
000:43:33.520 --> 000:43:41.560
Greg: oh. any question from me. maybe I'm wrong. Is this not the same thing as

463
000:43:43.250 --> 000:43:55.559
Greg:  And then this one I just get like confusing it.

464
000:43:56.730 --> 000:43:57.990
maybe just ignore it.

465
000:44:00.280 --> 000:44:01.740
Greg: Okay, cool.

466
000:44:02.170 --> 000:44:04.000
Greg: And let's see what happens.

467
000:44:07.420 --> 000:44:08.219
Greg: Sorry

468
000:44:10.110 --> 000:44:11.040
Greg: that out.

469
000:44:11.760 --> 000:44:14.469
Greg: And if you've done it a different way, also feel free to put on the chat.

470
000:44:14.790 --> 000:44:17.940
Greg: So anyone get that number, any chance?

471
000:44:22.180 --> 000:44:27.399
Greg: Okay, that feels about right for December 2021 chat on the screen

472
000:44:28.710 --> 000:44:30.389
Greg: get disrupted. I'm gonna have to make

473
000:44:30.420 --> 000:44:35.170
Greg: okay, cool. And then I think I'm gonna sort of round it off.

474
000:44:35.460 --> 000:44:38.890
Greg:  so this one again you could sort of like.

475
000:44:39.400 --> 000:44:46.099
Greg: I wondered if you go to a user's table again and you look at. Oh, no way sorry.

476
000:44:52.420 --> 000:44:54.439
Greg: Is this for the last one sherry?

477
000:44:55.270 --> 000:44:59.279
Shiri: Yeah, there was a different way of getting the same number.

478
000:44:59.700 --> 000:45:00.750
Greg: Okay, cool.

479
000:45:01.330 --> 000:45:04.139
Greg: That's also quite a nice way to think. I always like seeing that.

480
000:45:13.040 --> 000:45:20.879
Greg: Yeah, that a little bit simpler. So that's quite good. So you use extract instead trunk, and then you can just compare it to number. That's quite cool. And

481
000:45:22.540 --> 000:45:27.559
Greg: you could also, I think I don't even know if you actually have to extract the date, you know.

482
000:45:29.080 --> 000:45:31.550
Greg: I think if you come here you could

483
000:45:31.870 --> 000:45:34.570
Greg: news date and just select it equal to

484
000:45:37.220 --> 000:45:39.299
Greg: and see the day.

485
000:45:41.740 --> 000:45:43.940
Greg: So what would they have? How many purchases?

486
000:45:48.450 --> 000:45:56.570
Greg: Okay. So the only thing about this right is that we're gonna get the numbers. Oh, yeah, yeah. Sorry. I see.  yeah.

487
000:45:59.590 --> 000:46:01.769
Greg: Think you could actually even do

488
000:46:07.910 --> 000:46:12.370
Greg: just actually treat it as a string. If it's in the correct format.

489
000:46:16.150 --> 000:46:20.360
Greg: then you don't even have to extract anything which is the benefit of being a date function.

490
000:46:21.000 --> 000:46:23.239
Greg: Sorry that should be greater than or equal to.

491
000:46:25.590 --> 000:46:26.290
Greg: Yeah.

492
000:46:27.200 --> 000:46:30.499
Greg: I mean, we don't have 2023 data, but

493
000:46:32.770 --> 000:46:36.089
Greg: probably make sure we just treat for that as well.

494
000:46:45.180 --> 000:46:45.920
Let's see.

495
000:46:47.630 --> 000:46:48.920
Greg: produce the same thing.

496
000:46:49.370 --> 000:46:55.029
Greg: Okay, cool? Okay, cool. So yeah, if it's a date, you don't always even have to.

497
000:46:55.100 --> 000:46:56.290
Greg: So it's like that.

498
000:46:56.690 --> 000:47:05.329
Jack C: and cool, and pizza's put another way to doing it that way. How did you find the the latest date?

499
000:47:05.560 --> 000:47:07.900
You just order it to see when the

500
000:47:07.960 --> 000:47:12.959
Jack C: oh, I'm cheating. I just know that it ends at 2021. Is that what you mean?

501
000:47:13.030 --> 000:47:18.490
Jack C: Yeah, yeah, just to do it. That's like simplified way. How would you find out that date to

502
000:47:18.590 --> 000:47:27.590
Jack C: just select the Max date? And then look at I mean you. You don't even actually have to do it, because we said past months you could even define.

503
000:47:28.090 --> 000:47:29.080
Greg: And

504
000:47:30.670 --> 000:47:33.719
Greg: oh, I forgot what it's now you could use time diff. I think

505
000:47:35.640 --> 000:47:36.390
Greg: time.

506
000:47:38.820 --> 000:47:43.499
Greg: Yeah. So you could actually take latest purchase day is less than

507
000:47:44.590 --> 000:47:46.059
Greg: I might not get this right.

508
000:47:47.980 --> 000:47:48.890
Greg: And

509
000:47:50.620 --> 000:47:52.800
Greg: you can actually do something like that

510
000:47:54.790 --> 000:47:57.069
Greg: think this might be like date death.

511
000:48:02.340 --> 000:48:04.169
Greg: Maybe this needs to be in Max.

512
000:48:04.870 --> 000:48:17.860
Greg: Let me check that after, and I'll get back to you. There's a way of doing it dynamically, so that, like you can keep if it's being updated all the time, you can just like dynamically, look at the day. and I think that would need to be a Max or something. Let me let me double check that

513
000:48:17.900 --> 000:48:20.050
Greg: but yeah, yeah, fairly cheered.

514
000:48:20.140 --> 000:48:25.830
Greg: Your guys will way more of us. Okay, did anyone have a guy at the city that has the most purchases?

515
000:48:29.550 --> 000:48:31.840
Greg: It even just changes to most events.

516
000:48:33.790 --> 000:48:34.460
Greg: But

517
000:48:35.880 --> 000:48:37.749
Greg: if you could look at purchases.

518
000:48:40.470 --> 000:48:41.550
Greg: what's that called?

519
000:48:43.950 --> 000:48:45.240
Greg: That's a nice one.

520
000:48:48.260 --> 000:48:55.190
Greg: Yeah. I'm just gonna remove the limit. Only because I'm I'm interested. But yeah, if you just wanted the one with the most. Yeah.

521
000:48:55.420 --> 000:48:59.600
Greg:  

522
000:49:02.590 --> 000:49:04.850
Greg: think we just need to add an order by

523
000:49:10.990 --> 000:49:11.950
Greg: make sure.

524
000:49:16.430 --> 000:49:23.620
Greg: Okay, we're good. So one thing to think about, and I'm not going to get to do is now, but in terms of like more complicated business questions.

525
000:49:23.780 --> 000:49:26.589
Greg: It's got the most number of orders in these spaces.

526
000:49:26.970 --> 000:49:30.330
Greg: Something to think about is, if I change this.

527
000:49:30.910 --> 000:49:31.760
Greg: say.

528
000:49:32.260 --> 000:49:34.380
Greg: let's say, like we're sitting.

529
000:49:34.600 --> 000:49:36.249
Greg: it's the most profit. Spot

530
000:49:37.540 --> 000:49:39.860
Greg: comes a much more complicated question right?

531
000:49:42.210 --> 000:49:54.010
Greg: What might we need to do to begin to answer that question. I'm not thinking, SQL, just what things would you have to factor in as a business to start thinking about profitability that we haven't really looked at in this query.

532
000:49:57.790 --> 000:50:00.009
Greg: Haven't heard from Marsden today.

533
000:50:00.170 --> 000:50:06.730
Marsden Falcon: Just general expenses like rents and stuff like that where the shops are situated.

534
000:50:08.030 --> 000:50:14.079
Greg: Umhm, yeah. we don't have data on that, though. But it's a good. So I think actually, it's all online.

535
000:50:14.110 --> 000:50:22.640
Greg: So there aren't any shops, so it wouldn't be rent. But things like warehousing and stuff that still. So you're on about just Germany, my wife?

536
000:50:22.670 --> 000:50:28.030
Greg: No, no, I like so warehousing, and all of that distribution costs something we'd have to factor in.

537
000:50:28.740 --> 000:50:31.909
Greg: And what else about these orders?

538
000:50:31.940 --> 000:50:33.550
Greg: We look at all these tables.

539
000:50:35.760 --> 000:50:39.299
Greg: I'm deliberately not coming to Alex, cause you've spoken quite a lot today. I hope that's okay.

540
000:50:45.050 --> 000:50:50.919
Greg: Feel like there's people who haven't spoken who probably have the answer, but aren't sharing. But that's okay. I won't make you

541
000:50:51.970 --> 000:50:53.370
Greg: sherry. Do you want to go?

542
000:50:54.770 --> 000:50:58.950
Shiri: If you want to take into account the returns as well.

543
000:50:59.350 --> 000:51:00.869
Greg: Yeah, but sounds are a big one

544
000:51:02.020 --> 000:51:02.990
Greg: and

545
000:51:03.300 --> 000:51:06.029
returns. So you've got product returns over here.

546
000:51:06.320 --> 000:51:09.519
Greg: You've only got the transaction Id, so you'd need to join it to this table

547
000:51:10.290 --> 000:51:21.059
Greg: starts getting really complicated. How much did we spend on advertising? How did we get that user? So that actually, did they come naturally, or did we pay for them? Because we might be paying quite a lot of marketing expenses?

548
000:51:21.150 --> 000:51:29.650
Greg: So then you've got to look at your ad platform data. And you probably want to look at your session data so you can link back your user to

549
000:51:30.530 --> 000:51:36.259
Greg: their traffic source. And then you need to link that over here to see how much this traffic source is spending.

550
000:51:36.490 --> 000:51:38.000
Greg: Then you've also got like

551
000:51:38.080 --> 000:51:40.490
Greg: the actual cost of the item, but

552
000:51:41.030 --> 000:51:45.360
Greg: can think about that later. But yeah, returns and exchanges to bring in as well. And

553
000:51:46.200 --> 000:51:54.020
Greg: you can see how there's actually something, a lot of tables we need to join together. And this slide gets really complicated, because then you start thinking about well, what ids do we have in these tables?

554
000:51:54.230 --> 000:51:58.189
Greg: Sorry. I don't know why, it's split like this, can't. I'm just gonna do that.

555
000:51:58.930 --> 000:52:03.319
Greg: Yeah. God, yeah. No. Still come around. Come over here, my friend.

556
000:52:04.700 --> 000:52:17.459
Greg:  like that. Yeah. So just coming back. So you've got your cookie id. We don't know. All users are registered. So that's probably quite tricky. If you go to fun. Events. Same sort of thing here. Right?

557
000:52:18.140 --> 000:52:21.520
Greg:  go to our platform data.

558
000:52:21.760 --> 000:52:25.390
Greg: We can't link this back to the user directly.

559
000:52:25.960 --> 000:52:40.420
Greg: because it's by our platform. It's not by the user. So then we'd need to go to fun events. I think that's able to get it point, it gets really complicated. And this is sort of like what you spend quite a long time answering. It's not gonna be something you do in a couple of hours, and

560
000:52:40.960 --> 000:52:52.460
Greg: and it will be something that we look at and apply. But you can start seeing how like these queries can get quite complicated, and the dates you need to look at gets quite complicated, and so I do always think it's quite good to make tables containing the information you care about.

561
000:52:52.580 --> 000:52:59.299
Greg: and then it's like breaking it down into those little tables or separate problems, and that makes it easier to sort of target bigger ones.

562
000:53:00.600 --> 000:53:03.650
Greg: Okay, I think I'm gonna end a little bit early because I feel tired.

563
000:53:04.290 --> 000:53:05.359
Greg: I'm not anyone else.

564
000:53:05.520 --> 000:53:10.379
Greg: On a scale of one to 5. How are we feeling at the end of that section

565
000:53:11.200 --> 000:53:13.160
Greg: can do a hand, or you can put it in the chat.

566
000:53:15.640 --> 000:53:17.560
Greg: some twos and threes and forwards.

567
000:53:18.890 --> 000:53:22.409
Greg: It was a complicated. I'm totally okay with ones as well.

568
000:53:22.440 --> 000:53:25.400
Greg: and if you want to. DM me, and not put it publicly.

569
000:53:25.490 --> 000:53:34.259
Greg: That is also fine, too. and let me know what would sort of help watch the recording again. I'll share after this, and I'll share the queries.

570
000:53:34.950 --> 000:53:46.620
Greg: And just to emphasize that whole like window function was way more complicated than I wanted to get into. So please don't feel like too stressed about this, like everything we did today, was relatively complicated, and you'll have a lot more time to do it

571
000:53:46.690 --> 000:53:58.719
Greg: to do those sort of things if you ever need to in apply. So please don't stress. But hopefully it was useful and learned some new code. Please play around with that prison playground data set and create some tables in there if you want to.

572
000:53:58.890 --> 000:54:03.320
Greg: And and yeah, message me, if there's anything from the session where you're just like

573
000:54:04.290 --> 000:54:06.170
Greg: or I'd like to know more about.

574
000:54:06.370 --> 000:54:12.139
Greg: I don't think I was. I am recording. Oh, but yeah, otherwise have a good lunch, and I'll see you guys later. Thank you.

